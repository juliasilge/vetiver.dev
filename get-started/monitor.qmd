---
title: "Monitor"
format:
  html:
    toc: true
    include-after-body:
      - sync-tabs.html
---

Once a model is deployed, it is important to continue monitoring the model. Machine learning breaks quietly, that is, a model can continue running without error, even if it is performing poorly. Without monitoring for degradation, this can continue undiagnosed. Vetiver offers functions to quickly run, store, and plot model metrics. These functions are particularly suited to measure one model with multiple performance metrics over a period of time.

When a model is deployed, data comes in over time, even if time is not a feature for prediction. Even if your model does not explicitly use dates, the date can affect your model performance.

::: callout-tip
## What if my model is using time for prediction?

Sometimes your model uses datetime features for prediction. As we have defined it, monitoring ALWAYS includes datetime, not as a feature, but as a dimension along which we are monitoring.

:::

## Build a model

::: panel-tabset
## R

```{r}
#| code-fold: true
#| code-summary: "Show the code from previous steps"
#| message: false
library(tidymodels)
library(tidyverse)

car_mod <-
    workflow(mpg ~ ., linear_reg()) %>%
    fit(mtcars)
```

## Python

```{python}
#| code-fold: true
#| code-summary: "Show the code from previous steps"
#| message: false

from vetiver.data import mtcars
from sklearn import linear_model

car_mod = linear_model.LinearRegression().fit(mtcars.drop(columns="mpg"), mtcars["mpg"])
```
:::


## Compute metrics

You can compute multiple metrics at once over a certain time aggregation.

::: panel-tabset
## R

```{r}
#| message: false
library(vetiver)
cars <- read_csv("https://vetiver.rstudio.com/get-started/new-cars.csv")

metrics <-
    augment(car_mod, new_data = cars) %>%
    vetiver_compute_metrics(date_obs, "week", mpg, .pred, every = 1L)

original_metrics <- metrics[1:20,]

original_metrics
```


## Python

```{python}
import vetiver

import pandas as pd
from sklearn import metrics
from datetime import timedelta

cars = pd.read_csv("https://vetiver.rstudio.com/get-started/new-cars.csv")
cars["preds"] = car_mod.predict(cars.drop(columns=["date_obs", "mpg"]))

metric_set = [metrics.mean_absolute_error, metrics.mean_squared_error, metrics.r2_score]
td = timedelta(weeks = 1)

metrics = vetiver.compute_metrics(data = cars, 
                    date_var="date_obs", 
                    period = td, 
                    metric_set=metric_set, 
                    truth="mpg", 
                    estimate="preds")
original_metrics = metrics.iloc[:20, :]
               
original_metrics
```


:::

## Pin metrics

The first time you pin metrics, you can write to a board as normal. However, when adding new metrics to your pin, you may have dates that overlap with with the original pin, such as our `new_metrics` here. You can choose how to handle these dates by using the `overwrite` argument.

::: panel-tabset
## R

```{r}
#| message: false
library(pins)
board <- board_temp()
pin_write(board, original_metrics, "lm_fit_metrics")

new_metrics <- metrics[20:length(metrics),]

vetiver_pin_metrics(board, new_metrics, "lm_fit_metrics", overwrite = TRUE)
```



## Python

```{python}
import vetiver
import pins

board = pins.board_temp(allow_pickle_read = True)
board.pin_write(original_metrics, "lm_fit_metrics", type = "csv")

new_metrics = metrics.iloc[20:, :]
                    
vetiver.pin_metrics(board, new_metrics, "lm_fit_metrics", overwrite = True)

```

:::


## Plot metrics

With a metrics computed, visualizing metrics allows us to view the model performance.

::: panel-tabset
## R

```{r}
library(ggplot2)
vetiver_plot_metrics(new_metrics) +
    scale_size(range = c(2, 4))
```


## Python


```{python}
#| eval: false
p = vetiver.plot_metrics(df_metrics = new_metrics)
p.show()
```
![](../images/pythonmonitor.png)

:::

## Dashboarding

The vetiver package provides an R Markdown template for [creating a monitoring dashboard](https://rstudio.github.io/vetiver-r/reference/vetiver_dashboard.html) for pinned metrics. The template automates extracting some information from the metrics, and provides a framework to extend the dashboard with custom monitoring implementation.

